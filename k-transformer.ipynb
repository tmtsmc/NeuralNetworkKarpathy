{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T11:46:20.594772Z","iopub.execute_input":"2023-06-29T11:46:20.595154Z","iopub.status.idle":"2023-06-29T11:46:21.979928Z","shell.execute_reply.started":"2023-06-29T11:46:20.595123Z","shell.execute_reply":"2023-06-29T11:46:21.978752Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2023-06-29 11:46:21--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n\n2023-06-29 11:46:21 (16.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:46:59.204128Z","iopub.execute_input":"2023-06-29T11:46:59.204561Z","iopub.status.idle":"2023-06-29T11:46:59.213416Z","shell.execute_reply.started":"2023-06-29T11:46:59.204522Z","shell.execute_reply":"2023-06-29T11:46:59.210947Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(len(text))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:47:09.590983Z","iopub.execute_input":"2023-06-29T11:47:09.591380Z","iopub.status.idle":"2023-06-29T11:47:09.597192Z","shell.execute_reply.started":"2023-06-29T11:47:09.591349Z","shell.execute_reply":"2023-06-29T11:47:09.595903Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = len(chars)\nchars = sorted(list(set(text)))\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T12:18:44.267813Z","iopub.execute_input":"2023-06-29T12:18:44.268259Z","iopub.status.idle":"2023-06-29T12:18:44.297396Z","shell.execute_reply.started":"2023-06-29T12:18:44.268224Z","shell.execute_reply":"2023-06-29T12:18:44.296314Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nn = int(0.9*len(data))\ntrain_data = data[:n] # On sépare 90% en train data\nval_data = data[n:] # 10% en eval data","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:57:14.707832Z","iopub.execute_input":"2023-06-29T11:57:14.708325Z","iopub.status.idle":"2023-06-29T11:57:14.956365Z","shell.execute_reply.started":"2023-06-29T11:57:14.708294Z","shell.execute_reply":"2023-06-29T11:57:14.955091Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"block_size = 8 # context windows\ntrain_data[:block_size+1]","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:57:16.466174Z","iopub.execute_input":"2023-06-29T11:57:16.466530Z","iopub.status.idle":"2023-06-29T11:57:16.491003Z","shell.execute_reply.started":"2023-06-29T11:57:16.466503Z","shell.execute_reply":"2023-06-29T11:57:16.490172Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"{context} - {target}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:59:03.191162Z","iopub.execute_input":"2023-06-29T11:59:03.191533Z","iopub.status.idle":"2023-06-29T11:59:03.201402Z","shell.execute_reply.started":"2023-06-29T11:59:03.191506Z","shell.execute_reply":"2023-06-29T11:59:03.200017Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"tensor([18]) - 47\ntensor([18, 47]) - 56\ntensor([18, 47, 56]) - 57\ntensor([18, 47, 56, 57]) - 58\ntensor([18, 47, 56, 57, 58]) - 1\ntensor([18, 47, 56, 57, 58,  1]) - 15\ntensor([18, 47, 56, 57, 58,  1, 15]) - 47\ntensor([18, 47, 56, 57, 58,  1, 15, 47]) - 58\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel\nblock_size = 8 # context window\n\ndef get_batch(split):\n    # generate batch\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size):\n    for t in range(block_size):\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"{context.tolist()} - {target}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-29T12:04:28.931871Z","iopub.execute_input":"2023-06-29T12:04:28.932306Z","iopub.status.idle":"2023-06-29T12:04:28.992858Z","shell.execute_reply.started":"2023-06-29T12:04:28.932272Z","shell.execute_reply":"2023-06-29T12:04:28.991251Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\n[24] - 43\n[24, 43] - 58\n[24, 43, 58] - 5\n[24, 43, 58, 5] - 57\n[24, 43, 58, 5, 57] - 1\n[24, 43, 58, 5, 57, 1] - 46\n[24, 43, 58, 5, 57, 1, 46] - 43\n[24, 43, 58, 5, 57, 1, 46, 43] - 39\n[44] - 53\n[44, 53] - 56\n[44, 53, 56] - 1\n[44, 53, 56, 1] - 58\n[44, 53, 56, 1, 58] - 46\n[44, 53, 56, 1, 58, 46] - 39\n[44, 53, 56, 1, 58, 46, 39] - 58\n[44, 53, 56, 1, 58, 46, 39, 58] - 1\n[52] - 58\n[52, 58] - 1\n[52, 58, 1] - 58\n[52, 58, 1, 58] - 46\n[52, 58, 1, 58, 46] - 39\n[52, 58, 1, 58, 46, 39] - 58\n[52, 58, 1, 58, 46, 39, 58] - 1\n[52, 58, 1, 58, 46, 39, 58, 1] - 46\n[25] - 17\n[25, 17] - 27\n[25, 17, 27] - 10\n[25, 17, 27, 10] - 0\n[25, 17, 27, 10, 0] - 21\n[25, 17, 27, 10, 0, 21] - 1\n[25, 17, 27, 10, 0, 21, 1] - 54\n[25, 17, 27, 10, 0, 21, 1, 54] - 39\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module) :\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx) # (B=batch_size, T=context_window, C=vocab_size)\n        if targets is None :\n            loss = None\n        else : \n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T)\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            logits = logits[:, -1, :] # (B, C)\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n    \nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\nidx = torch.zeros((1, 1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T12:27:26.015894Z","iopub.execute_input":"2023-06-29T12:27:26.016350Z","iopub.status.idle":"2023-06-29T12:27:26.065006Z","shell.execute_reply.started":"2023-06-29T12:27:26.016316Z","shell.execute_reply":"2023-06-29T12:27:26.063535Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # Optimizer with learning rate","metadata":{"execution":{"iopub.status.busy":"2023-06-29T12:29:42.019902Z","iopub.execute_input":"2023-06-29T12:29:42.020347Z","iopub.status.idle":"2023-06-29T12:29:42.026869Z","shell.execute_reply.started":"2023-06-29T12:29:42.020314Z","shell.execute_reply":"2023-06-29T12:29:42.025098Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nfor steps in range(10000):\n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2023-06-29T12:33:08.295427Z","iopub.execute_input":"2023-06-29T12:33:08.295925Z","iopub.status.idle":"2023-06-29T12:33:24.520466Z","shell.execute_reply.started":"2023-06-29T12:33:08.295888Z","shell.execute_reply":"2023-06-29T12:33:24.519469Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"2.4606239795684814\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T12:33:40.709172Z","iopub.execute_input":"2023-06-29T12:33:40.709925Z","iopub.status.idle":"2023-06-29T12:33:40.730441Z","shell.execute_reply.started":"2023-06-29T12:33:40.709890Z","shell.execute_reply":"2023-06-29T12:33:40.729421Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\nEThand t, aghath,\nMI: Fol'storevend os tho geat; tisheer lid al'st bal l are, tour bu olll pt my.\nFR\n","output_type":"stream"}]}]}